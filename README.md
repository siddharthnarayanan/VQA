# Visual Question Answering

A significant big gap still exists in the quality of captions as compared to humans. The goal of this project was to make machines to understand images, question and answers correspond to the images. Inputs were taken in the form of either images, questions & answers, and result in a corresponding combination of outputs. We explored the aspects of visual question answering (VQA) that deal with teaching/testing a machine to understand images and embedded data and generated a VQA scheme through a deep multimodal similarity model (DMSM) to compute the cosine measure between images and multiple-choice/open-ended descriptive text. Using DMSM, we conducted experiments with different experimental settings comprising of clipart images and/or natural images.

For a detailed read, check out the [article](https://medium.com/@siddharthnarayanan7/visual-question-answering-with-various-feature-combinations-extensions-of-visual-question-940edbd84d82)
